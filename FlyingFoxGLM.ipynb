{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74fe623-2a3b-4234-b8c8-de9dc8780132",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# general\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "from importlib import reload\n",
    "\n",
    "# modeling\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import shap\n",
    "\n",
    "# custom\n",
    "from scripts import HyperparameterTuning as HT\n",
    "from scripts import Plots, Metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "387b5115-94a5-4545-b64d-827b14a829d0",
   "metadata": {},
   "source": [
    "# Load and pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789d4950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "env_path = 'data/environmental_data.csv'\n",
    "bat_path = 'data/bat-level_data.csv'\n",
    "random_state = 1337\n",
    "dataset = 'env' # 'env', 'bat'\n",
    "target = 'shortage'\n",
    "\n",
    "# load configs\n",
    "env_features = []\n",
    "with open('config/env_features.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        env_features.append(line.strip())\n",
    "bat_features = []\n",
    "with open('config/bat_features.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        bat_features.append(line.strip())\n",
    "rename = {}\n",
    "with open('config/rename.tsv', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split('\\t')\n",
    "        rename[line[0]] = line[1]\n",
    "    rename['Intercept'] = 'Intercept'\n",
    "\n",
    "# book keeping\n",
    "features = env_features if dataset == 'env' else bat_features\n",
    "\n",
    "# load both datasets, sort, and merge\n",
    "df_env = pd.read_csv(env_path).sort_values(\n",
    "    by=['cal_year', 'cal_month'], ascending=[True, True])\n",
    "df_bat = pd.read_csv(bat_path).sort_values(\n",
    "    by=['cal_year', 'cal_month'], ascending=[True, True])\n",
    "df = pd.merge(df_env, df_bat, on=['cal_year', 'cal_month'], how='outer')\n",
    "\n",
    "# convert categories to binary and drop missing\n",
    "df = df.replace('shortage', 1).replace('not_shortage', 0)\n",
    "df = df.dropna(subset=[target])\n",
    "df[target] = df[target].astype(int)\n",
    "\n",
    "# include season as additional feature\n",
    "to_season = {\n",
    "    12: 1,  1: 1,  2: 1,\n",
    "     3: 2,  4: 2,  5: 2,\n",
    "     6: 3,  7: 3,  8: 3,\n",
    "     9: 4, 10: 4, 11: 4,}\n",
    "df['cal_season'] = df['cal_month'].apply(lambda x: to_season[x]).astype(float)\n",
    "\n",
    "# drop rows with >50% missing features\n",
    "df = df.dropna(thresh=0.5*len(features), subset=features)\n",
    "df_missing = df.copy()\n",
    "\n",
    "# impute missing values\n",
    "n_missing = df[features].isna().sum().sum()\n",
    "n_total = len(df) * len(features)\n",
    "imp = IterativeImputer(max_iter=100, random_state=random_state)\n",
    "df[features] = imp.fit_transform(df[features])\n",
    "\n",
    "# start integer features from 0 and convert to categorical\n",
    "for feature in ['cal_year', 'cal_month']:\n",
    "    df[feature] = df[feature].astype(int)\n",
    "for feature in ['cal_season', 'cal_month']:\n",
    "    df[feature] = (df[feature] - df[feature].min()).astype('category')\n",
    "\n",
    "# reset df index starting from 0\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# summary\n",
    "print('Missing proportion: {:.2f}% values'.format(n_missing / n_total * 100))\n",
    "print('Data shape: {} months, {} features'.format(*df[features].shape))\n",
    "print('Food shortages: {} / {} ({:.2f}%)'.format(\n",
    "    df[target].sum(), len(df), df[target].sum() / len(df) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6890bdb",
   "metadata": {},
   "source": [
    "# Split data into train/val/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6485faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "rolling_forecast = True\n",
    "start_year = df['cal_year'].min() + 4\n",
    "test_year = 2018\n",
    "split_size = 2\n",
    "\n",
    "# convert dataframes to numpy arrays for modeling\n",
    "X, y = df[features].values, df[target].values\n",
    "years = df['cal_year'].values\n",
    "\n",
    "# get indices of training and validation sets\n",
    "split_years = np.arange(start_year, test_year, split_size)\n",
    "if rolling_forecast:\n",
    "    train_idx = np.argwhere(years < test_year).flatten()\n",
    "else:\n",
    "    train_idx = np.argwhere((years >= test_year - 4) & (years < test_year)).flatten()\n",
    "train_idxs, val_idxs = [], []\n",
    "for i, sy in enumerate(split_years):\n",
    "    if rolling_forecast:\n",
    "        train_idxs.append(np.argwhere(years < sy).flatten())\n",
    "    else:\n",
    "        train_idxs.append(np.argwhere((years >= sy-4) & (years < sy)).flatten())\n",
    "    val_idxs.append(np.argwhere((years >= sy) * (years < sy+split_size)).flatten())\n",
    "test_idx = np.argwhere(years >= test_year).flatten()\n",
    "\n",
    "# compute train/val score weights\n",
    "weights = [len(t_idx) for t_idx in train_idxs]\n",
    "\n",
    "# plot train/val/test splits\n",
    "Plots.cv_splits(\n",
    "    years, \n",
    "    cv_splits=[train_idxs, val_idxs, test_idx], \n",
    "    dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79762712",
   "metadata": {},
   "source": [
    "# Train GLM using hyperparameter grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fae5f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress PerfectSeparationWarning from statsmodels\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \n",
    "    category=statsmodels.tools.sm_exceptions.PerfectSeparationWarning)\n",
    "\n",
    "# custom GLM classifier mimicking LightGBMClassifier\n",
    "class GLMClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        add_intercept=True,\n",
    "        max_iter=100,\n",
    "        random_state=42\n",
    "    ):\n",
    "        \n",
    "        # initialize\n",
    "        self.add_intercept = add_intercept\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self.result_ = None\n",
    "        self.eps = np.sqrt(np.finfo(float).eps)\n",
    "\n",
    "        # copy lgb evaluation metric for hyperparameter tuning script\n",
    "        self.evals_result_ = {'valid_0': {'binary_logloss': []}}\n",
    "\n",
    "    def fit(self, X, y, eval_set=None, **kwargs):\n",
    "        \n",
    "        # optional intercept\n",
    "        X = sm.add_constant(X, prepend=True) if self.add_intercept else X\n",
    "\n",
    "        # logistic regression GLM\n",
    "        model = sm.GLM(\n",
    "            y,\n",
    "            X,\n",
    "            family=sm.families.Binomial()\n",
    "        )\n",
    "\n",
    "        # fit GLM\n",
    "        self.result_ = model.fit(maxiter=self.max_iter, **kwargs)\n",
    "\n",
    "        # validation eval (if provided)\n",
    "        if eval_set is not None:\n",
    "            X_val, y_val = eval_set\n",
    "            X_val = sm.add_constant(X_val, prepend=True) if self.add_intercept else X_val\n",
    "            val_loss = log_loss(y_val, self.predict_proba(X_val)[:, 1], labels=[0, 1])\n",
    "        else:\n",
    "            val_loss = np.nan\n",
    "        self.evals_result_['valid_0']['binary_logloss'].append(val_loss)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        # preprocess\n",
    "        X = sm.add_constant(X, prepend=True) if self.add_intercept else X\n",
    "\n",
    "        # logistic regression returns p = P(y=1)\n",
    "        p = self.result_.predict(X)\n",
    "        p = np.clip(p, self.eps, 1 - self.eps)  # numeric stability\n",
    "        return np.column_stack([1 - p, p])\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        probs = self.predict_proba(X)[:, 1]\n",
    "        return (probs >= threshold).astype(int)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        \n",
    "        # check previously stored validation losses\n",
    "        val_losses = self.evals_result_['valid_0']['binary_logloss']\n",
    "        valid_numeric_losses = [v for v in val_losses if not np.isnan(v)]\n",
    "\n",
    "        # compute log loss on given data if no valid logs are found\n",
    "        if len(valid_numeric_losses) > 0:\n",
    "            return min(valid_numeric_losses)\n",
    "        else:\n",
    "            if self.add_intercept:\n",
    "                X = sm.add_constant(X, prepend=True)\n",
    "            p = self.predict_proba(X)[:, 1]\n",
    "            return log_loss(y, p, labels=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ab69f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# tune hyperparameters\n",
    "#\n",
    "\n",
    "model_class = GLMClassifier\n",
    "\n",
    "# hyperparameter grid\n",
    "model_hyper = {\n",
    "    'add_intercept': [True, False],\n",
    "    'max_iter': list(np.arange(100)),\n",
    "}\n",
    "model_fixed = {\n",
    "    'random_state': 42,\n",
    "}\n",
    "\n",
    "# Training hyperparams can remain empty if you have no extra parameters\n",
    "train_hyper = {}\n",
    "train_fixed = {}\n",
    "\n",
    "model_best, train_best, model_list, score_list = HT.hyperparameter_tuning(\n",
    "    model_class=model_class,\n",
    "    model_hyper=model_hyper,\n",
    "    model_fixed=model_fixed,\n",
    "    train_hyper=train_hyper,\n",
    "    train_fixed=train_fixed,\n",
    "    x=X,\n",
    "    y=y,\n",
    "    train_indices=train_idxs,\n",
    "    val_indices=val_idxs,\n",
    "    weights=None,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"Best model hyperparameters:\", model_best)\n",
    "print(\"Best training hyperparameters:\", train_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6441b9",
   "metadata": {},
   "source": [
    "# Plot model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c7943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(Plots)\n",
    "\n",
    "def predict(train_idx, val_idx):\n",
    "    \n",
    "    # fit model\n",
    "    model = model_class(\n",
    "        **model_best, \n",
    "        **model_fixed,\n",
    "    ).fit(\n",
    "        X=X[train_idx],\n",
    "        y=y[train_idx],\n",
    "        eval_set=(X[val_idx], y[val_idx]),\n",
    "        **train_best,\n",
    "        **train_fixed,\n",
    "    )\n",
    "\n",
    "    # predict probabilities\n",
    "    y_pred_train = model.predict_proba(\n",
    "        X[train_idx], \n",
    "    )[:, 1]\n",
    "    y_pred_val = model.predict_proba(\n",
    "        X[val_idx], \n",
    "    )[:, 1]\n",
    "\n",
    "    return y_pred_train, y_pred_val, model\n",
    "\n",
    "# compute predictions for each set\n",
    "y_prob_train = [predict(t, v)[0] for t, v in zip(train_idxs, val_idxs)]\n",
    "y_prob_train.append(predict(train_idx, test_idx)[0])\n",
    "y_prob_val = np.concatenate([predict(t, v)[1] for t, v in zip(train_idxs, val_idxs)])\n",
    "y_prob_test = predict(np.arange(test_idx[0]), test_idx)[1]\n",
    "y_prob = np.concatenate([y_prob_train[0], y_prob_val, y_prob_test])\n",
    "\n",
    "# create dates from years and months\n",
    "dates = df['cal_year'].astype(str) + '-' + df['cal_month'].astype(int).astype(str)\n",
    "dates = pd.Series(dates.values, index=np.arange(len(dates)))\n",
    "\n",
    "# compute optimal probability threshold based on f1 score\n",
    "val_idx = np.concatenate(val_idxs)\n",
    "threshold = Metrics.get_threshold(y[val_idx], y_prob_val)\n",
    "y_pred = (y_prob > threshold).astype(int)\n",
    "\n",
    "# plot predictions over time\n",
    "bd = 'bd'[dataset == 'bat']\n",
    "Plots.predictions(\n",
    "    y_true=y, \n",
    "    y_probs=[y_prob_train[0], y_prob_val, y_prob_test],\n",
    "    cv_splits=[train_idxs, val_idxs, test_idx],\n",
    "    dates=dates, \n",
    "    threshold=threshold, \n",
    "    dataset=dataset,\n",
    "    save_name=f'figures/Fig1{bd}.pdf',\n",
    ")\n",
    "\n",
    "# plot train/val/test splits separately\n",
    "bd = 'bd'[dataset == 'bat']\n",
    "Plots.predictions_subplots(\n",
    "    y_true=y, \n",
    "    y_probs=[y_prob_train, y_prob_val, y_prob_test],\n",
    "    cv_splits=[train_idxs+[train_idx], val_idxs, test_idx],\n",
    "    dates=dates, \n",
    "    threshold=threshold, \n",
    "    dataset=dataset,\n",
    "    save_name=f'figures/SI_Fig4{bd}.pdf' if rolling_forecast else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8be3f7e",
   "metadata": {},
   "source": [
    "# Print model performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f7aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(Metrics)\n",
    "Metrics.print_metrics(\n",
    "    y, \n",
    "    y_prob, \n",
    "    dates.values, \n",
    "    val_idx=val_idx, \n",
    "    test_idx=test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d05b2f",
   "metadata": {},
   "source": [
    "# Plot feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec93926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# options\n",
    "keep = 11 if dataset == 'env' else 5\n",
    "# keep = len(features) + 1\n",
    "ab = 'ab'[dataset == 'bat']\n",
    "\n",
    "# add intercept to features if it was optimal\n",
    "if model_best['add_intercept']:\n",
    "    _features = ['Intercept'] + features\n",
    "else:\n",
    "    _features = features\n",
    "\n",
    "# train model on full training set\n",
    "model = predict(np.arange(test_idx[0]), test_idx)[-1]\n",
    "\n",
    "# extract values\n",
    "params = model.result_.params     # coefficient estimates\n",
    "tvalues = model.result_.tvalues   # t-values for each coefficient\n",
    "\n",
    "# compute odds ratios\n",
    "odds_ratios = np.exp(params)\n",
    "or_df = pd.DataFrame({\n",
    "    'Feature': [rename[feature] for feature in _features],\n",
    "    'OddsRatio': odds_ratios,\n",
    "    'tValue': tvalues\n",
    "}).sort_values('OddsRatio', ascending=True)\n",
    "\n",
    "# compute marginal effects\n",
    "marg_eff = model.result_.get_margeff()\n",
    "marg_eff_df = marg_eff.summary_frame()  # includes dy/dx, std err, z, etc.\n",
    "marg_eff_df['Feature'] = [rename[feature] for feature in _features]\n",
    "marg_eff_df = marg_eff_df.reset_index(drop=True).sort_values('dy/dx', ascending=True)\n",
    "\n",
    "# initialize figure\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 0.5*keep))\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# plot odds ratios\n",
    "axs[0].errorbar(\n",
    "    x=or_df['OddsRatio'].values[-keep:],\n",
    "    y=or_df['Feature'].values[-keep:],\n",
    "    fmt='o',\n",
    "    color='blue',\n",
    "    ecolor='gray',\n",
    "    capsize=3\n",
    ")\n",
    "axs[0].axvline(x=1, color='red', linestyle='--')  # Reference line at OR=1\n",
    "axs[0].set_xlabel('Odds ratio', fontsize=12)\n",
    "axs[0].tick_params(axis='x', labelsize=10)\n",
    "axs[0].tick_params(axis='y', labelsize=10)\n",
    "\n",
    "# plot marginal effects\n",
    "axs[1].errorbar(\n",
    "    x=marg_eff_df['dy/dx'].values[-keep:],\n",
    "    y=marg_eff_df['Feature'].values[-keep:],\n",
    "    fmt='o',\n",
    "    color='blue',\n",
    "    ecolor='gray',\n",
    "    capsize=3\n",
    ")\n",
    "axs[1].axvline(x=0, color='red', linestyle='--')  # 0 line for marginal effects\n",
    "axs[1].set_xlabel('Marginal effect on probability', fontsize=12)\n",
    "if all(marg_eff_df['Feature'].values[-keep:] == or_df['Feature'].values[-keep:]):\n",
    "    axs[1].set_yticklabels([]) # hide feature names if they match\n",
    "axs[1].tick_params(axis='x', labelsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'figures/SI_Fig6{ab}.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flying-fox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

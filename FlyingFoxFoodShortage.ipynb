{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74fe623-2a3b-4234-b8c8-de9dc8780132",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# general\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "# modeling\n",
    "import lightgbm as lgb\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import shap\n",
    "\n",
    "# custom\n",
    "from scripts import HyperparameterTuning as HT\n",
    "from scripts import Plots, Metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "387b5115-94a5-4545-b64d-827b14a829d0",
   "metadata": {},
   "source": [
    "# Load and pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789d4950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "env_path = 'data/environmental_data.csv'\n",
    "bat_path = 'data/bat-level_data.csv'\n",
    "random_state = 1337\n",
    "dataset = 'bat' # 'env', 'bat'\n",
    "target = 'shortage'\n",
    "\n",
    "# load configs\n",
    "env_features = []\n",
    "with open('config/env_features.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        env_features.append(line.strip())\n",
    "bat_features = []\n",
    "with open('config/bat_features.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        bat_features.append(line.strip())\n",
    "rename = {}\n",
    "with open('config/rename.tsv', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split('\\t')\n",
    "        rename[line[0]] = line[1]\n",
    "\n",
    "# book keeping\n",
    "features = env_features if dataset == 'env' else bat_features\n",
    "\n",
    "# load both datasets, sort, and merge\n",
    "df_env = pd.read_csv(env_path).sort_values(\n",
    "    by=['cal_year', 'cal_month'], ascending=[True, True])\n",
    "df_bat = pd.read_csv(bat_path).sort_values(\n",
    "    by=['cal_year', 'cal_month'], ascending=[True, True])\n",
    "df = pd.merge(df_env, df_bat, on=['cal_year', 'cal_month'], how='outer')\n",
    "\n",
    "# convert categories to binary and drop missing\n",
    "df = df.replace('shortage', 1).replace('not_shortage', 0)\n",
    "df = df.dropna(subset=[target])\n",
    "df[target] = df[target].astype(int)\n",
    "\n",
    "# include season as additional feature\n",
    "to_season = {\n",
    "    12: 1,  1: 1,  2: 1,\n",
    "     3: 2,  4: 2,  5: 2,\n",
    "     6: 3,  7: 3,  8: 3,\n",
    "     9: 4, 10: 4, 11: 4,}\n",
    "df['cal_season'] = df['cal_month'].apply(lambda x: to_season[x]).astype(float)\n",
    "\n",
    "# drop rows with >50% missing features\n",
    "df = df.dropna(thresh=0.5*len(features), subset=features)\n",
    "\n",
    "# impute missing values\n",
    "n_missing = df[features].isna().sum().sum()\n",
    "n_total = len(df) * len(features)\n",
    "imp = IterativeImputer(max_iter=100, random_state=random_state)\n",
    "df[features] = imp.fit_transform(df[features])\n",
    "\n",
    "# start integer features from 0 and convert to categorical\n",
    "for feature in ['cal_year', 'cal_month']:\n",
    "    df[feature] = df[feature].astype(int)\n",
    "for feature in ['cal_season', 'cal_month']:\n",
    "    df[feature] = (df[feature] - df[feature].min()).astype('category')\n",
    "\n",
    "# reset df index starting from 0\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# summary\n",
    "print('Missing proportion: {:.2f}% values'.format(n_missing / n_total * 100))\n",
    "print('Data shape: {} months, {} features'.format(*df[features].shape))\n",
    "print('Food shortages: {} / {} ({:.2f}%)'.format(\n",
    "    df[target].sum(), len(df), df[target].sum() / len(df) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6890bdb",
   "metadata": {},
   "source": [
    "# Split data into train/val/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6485faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "start_year = df['cal_year'].min() + 4\n",
    "test_year = 2018\n",
    "split_size = 2\n",
    "\n",
    "# convert dataframes to numpy arrays for modeling\n",
    "X, y = df[features].values, df[target].values\n",
    "years = df['cal_year'].values\n",
    "\n",
    "# get indices of training and validation sets\n",
    "split_years = np.arange(start_year, test_year, split_size)\n",
    "train_idxs, val_idxs = [], []\n",
    "for i, sy in enumerate(split_years):\n",
    "    train_idxs.append(np.argwhere(years < sy).flatten())\n",
    "    val_idxs.append(np.argwhere((years >= sy) * (years < sy+split_size)).flatten())\n",
    "test_idx = np.argwhere(years >= test_year).flatten()\n",
    "\n",
    "# compute train/val score weights\n",
    "weights = [len(t_idx) for t_idx in train_idxs]\n",
    "\n",
    "# plot train/val/test splits\n",
    "Plots.cv_splits(\n",
    "    years, \n",
    "    cv_splits=[train_idxs, val_idxs, test_idx], \n",
    "    dataset=dataset,\n",
    "    save_name=f'figures/{dataset}_cv_splits.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79762712",
   "metadata": {},
   "source": [
    "# Train GBDT using hyperparameter grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa55250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "np.random.seed(random_state)\n",
    "use_best = False\n",
    "\n",
    "# model class\n",
    "model_class = lgb.LGBMClassifier\n",
    "\n",
    "# add eval method to model class (minimum validation log loss)\n",
    "def evaluate(self, x, y):\n",
    "    return min(self.evals_result_['valid_0']['binary_logloss'])\n",
    "model_class.evaluate = evaluate\n",
    "\n",
    "# model parameters\n",
    "model_hyper = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1], # boosting learning rate   (default=0.1)\n",
    "    'num_leaves': [4, 8, 16], # max tree leaves for base learners  (default=31)\n",
    "    'max_bin': [4, 8, 16], # max bins that feature is bucketed     (default=255)\n",
    "    'min_child_samples': [1, 5, 10], # min data in leaf            (default=20)\n",
    "    'min_child_weight': [0.001, 0.01, 0.1], # min sum leaf hessian (default=1e-3)\n",
    "    'reg_alpha': [0.0, 0.01, 0.1], # l1 regularization             (default=0.0)\n",
    "    'reg_lambda': [0.0, 0.01, 0.1], # l2 regularization            (default=0.0)\n",
    "}\n",
    "model_fixed = {\n",
    "    'boosting_type': 'gbdt', # gradient boosting decision tree\n",
    "    'objective': 'binary', # shortage / no shortage binary classification\n",
    "    'n_estimators': 100, # number of iterations\n",
    "    'class_weight': 'balanced', # include class balance in loss function\n",
    "    'importance_type': 'split', # feature importance by number of splits\n",
    "    'n_jobs': 1, # -1: use all cores (wasteful on small datasets like this one)\n",
    "    'max_depth': 4, # constrain max tree depth to prevent overfitting\n",
    "    'cat_l2': 0, # L2 regularization for categorical features\n",
    "    'cat_smooth': 0, # smoothing for categorical features\n",
    "    'min_split_gain': 0.0, # min. reduction in loss from adding a split\n",
    "    'verbose': -1, # -1 = silent, 0 = warn, 1 = info\n",
    "    'random_state': random_state, # seed\n",
    "}\n",
    "\n",
    "# train parameters\n",
    "train_hyper = {}\n",
    "train_fixed = {\n",
    "    'feature_name': 'auto', # 'None', 'auto'\n",
    "    'categorical_feature': None, # None, 'auto'\n",
    "    'eval_metric': model_fixed['objective'],\n",
    "    'callbacks': [ # early stopping so lgbm records validation performance\n",
    "        lgb.early_stopping(model_fixed['n_estimators'],\n",
    "        verbose=False)], \n",
    "}\n",
    "\n",
    "# best parameters\n",
    "env_best = {\n",
    "    'learning_rate':     [0.1],\n",
    "    'num_leaves':        [16],\n",
    "    'max_bin':           [4],\n",
    "    'min_child_samples': [5],\n",
    "    'min_child_weight':  [0.01],\n",
    "    'reg_alpha':         [0.0],\n",
    "    'reg_lambda':        [0.01],\n",
    "}\n",
    "bat_best = {\n",
    "    'learning_rate':     [0.1],\n",
    "    'num_leaves':        [8],\n",
    "    'max_bin':           [16],\n",
    "    'min_child_samples': [5],\n",
    "    'min_child_weight':  [0.01],\n",
    "    'reg_alpha':         [0.0],\n",
    "    'reg_lambda':        [0.1],\n",
    "}\n",
    "if use_best and dataset == 'env': model_hyper = env_best\n",
    "if use_best and dataset == 'bat': model_hyper = bat_best\n",
    "\n",
    "# tune model hyperparameters\n",
    "model_best, train_best, model_list, score_list = HT.hyperparameter_tuning(\n",
    "    model_class=model_class,\n",
    "    model_hyper=model_hyper,\n",
    "    model_fixed=model_fixed,\n",
    "    train_hyper=train_hyper,\n",
    "    train_fixed=train_fixed,\n",
    "    x=X,\n",
    "    y=y,\n",
    "    train_indices=train_idxs,\n",
    "    val_indices=val_idxs,\n",
    "    weights=weights,\n",
    "    verbose=True,\n",
    ")\n",
    "time.sleep(0.5)\n",
    "\n",
    "# best iteration is that which minimizes the validation score\n",
    "def get_best_iter(m): \n",
    "    return np.nanargmin(m.evals_result_['valid_0']['binary_logloss'])\n",
    "\n",
    "# get best iteration\n",
    "score_means = [np.average(s, weights=weights) for s in score_list]\n",
    "best_models = model_list[np.nanargmin(score_means)]\n",
    "iterations = [get_best_iter(m) for m in best_models]\n",
    "best_iteration = int(np.average(iterations, weights=weights))\n",
    "model_fixed['n_estimators'] = best_iteration\n",
    "print('Best iteration:', best_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6441b9",
   "metadata": {},
   "source": [
    "# Plot model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c7943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(train_idx, val_idx):\n",
    "    \n",
    "    # fit model\n",
    "    model = model_class(\n",
    "        **model_best, \n",
    "        **model_fixed,\n",
    "    ).fit(\n",
    "        X=X[train_idx],\n",
    "        y=y[train_idx],\n",
    "        eval_set=(X[val_idx], y[val_idx]),\n",
    "        **train_best,\n",
    "        **train_fixed,\n",
    "    )\n",
    "\n",
    "    # predict probabilities\n",
    "    y_pred_train = model.predict_proba(\n",
    "        X[train_idx], num_iteration=best_iteration)[:, 1]\n",
    "    y_pred_val = model.predict_proba(\n",
    "        X[val_idx], num_iteration=best_iteration)[:, 1]\n",
    "\n",
    "    return y_pred_train, y_pred_val, model\n",
    "\n",
    "# compute predictions for each set\n",
    "y_prob_train = predict(train_idxs[0], val_idxs[0])[0]\n",
    "y_prob_val = np.concatenate([predict(t, v)[1] for t, v in zip(train_idxs, val_idxs)])\n",
    "y_prob_test = predict(np.arange(test_idx[0]), test_idx)[1]\n",
    "y_prob = np.concatenate([y_prob_train, y_prob_val, y_prob_test])\n",
    "\n",
    "# create dates from years and months\n",
    "dates = df['cal_year'].astype(str) + '-' + df['cal_month'].astype(int).astype(str)\n",
    "dates = pd.Series(dates.values, index=np.arange(len(dates)))\n",
    "\n",
    "# compute optimal probability threshold based on f1 score\n",
    "val_idx = np.concatenate(val_idxs)\n",
    "threshold = Metrics.get_threshold(y[val_idx], y_prob_val)\n",
    "y_pred = (y_prob > threshold).astype(int)\n",
    "\n",
    "# plot predictions over time\n",
    "Plots.predictions(\n",
    "    y_true=y, \n",
    "    y_probs=[y_prob_train, y_prob_val, y_prob_test],\n",
    "    cv_splits=[train_idxs, val_idxs, test_idx],\n",
    "    dates=dates, \n",
    "    threshold=threshold, \n",
    "    dataset=dataset,\n",
    "    save_name=f'figures/{dataset}_predictions.pdf',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8be3f7e",
   "metadata": {},
   "source": [
    "# Print model performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f7aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metrics.print_metrics(\n",
    "    y, \n",
    "    y_prob, \n",
    "    dates.values, \n",
    "    val_idx=val_idx, \n",
    "    test_idx=test_idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf23abed",
   "metadata": {},
   "source": [
    "# Compute shap values and shap interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872f02d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# book keeping\n",
    "train_idx = np.argwhere(years < test_year).flatten()\n",
    "test_idx = np.argwhere(years >= test_year).flatten()\n",
    "\n",
    "# split datasets\n",
    "x_train = pd.DataFrame(X[train_idx], columns=[rename[f] for f in features])\n",
    "y_train = y[train_idx]\n",
    "x_test = pd.DataFrame(X[test_idx], columns=[rename[f] for f in features])\n",
    "y_test = y[test_idx]\n",
    "x_total = pd.DataFrame(X, columns=[rename[f] for f in features])\n",
    "y_total = y\n",
    "\n",
    "# prefit GBM Classifier on training set\n",
    "gbdt = predict(train_idx, test_idx)[2]\n",
    "\n",
    "# compute shap values [N, F]\n",
    "explainer = shap.Explainer(gbdt, algorithm='tree', seed=random_state)\n",
    "train_shap_values = explainer(x_train)[:, :, 1] # 1 = shortage\n",
    "test_shap_values = explainer(x_test)[:, :, 1] # 1 = shortage\n",
    "\n",
    "# compute SHAP interactions [N, F, F]\n",
    "train_shap_interactions = explainer.shap_interaction_values(x_train)\n",
    "test_shap_interactions = explainer.shap_interaction_values(x_test)\n",
    "\n",
    "# normalize shap values [F]\n",
    "train_shap_norms = np.abs(train_shap_values.values).mean(0)\n",
    "train_shap_norms = train_shap_norms / train_shap_norms.sum()\n",
    "test_shap_norms = np.abs(test_shap_values.values).mean(0)\n",
    "test_shap_norms = test_shap_norms / test_shap_norms.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f94136",
   "metadata": {},
   "source": [
    "# Plot shap values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d6da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "max_display = (train_shap_norms > 0.025).sum()\n",
    "order = list(np.argsort(train_shap_norms)[::-1])\n",
    "cmap = 'coolwarm'\n",
    "\n",
    "# plot shap values for training set\n",
    "Plots.shap_beeswarm(\n",
    "    shap_values=train_shap_values, \n",
    "    max_display=max_display,\n",
    "    split='train',\n",
    "    order=order, \n",
    "    save_name=f'figures/{dataset}_shap_values_train.pdf',\n",
    ")\n",
    "\n",
    "# plot shap values for test set\n",
    "Plots.shap_beeswarm(\n",
    "    shap_values=test_shap_values, \n",
    "    max_display=max_display,\n",
    "    split='test',\n",
    "    order=order, \n",
    "    save_name=f'figures/{dataset}_shap_values_test.pdf',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7efb2",
   "metadata": {},
   "source": [
    "# Plot shap interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d86353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top two feature names, values, shaps, and interactions\n",
    "f1 = features[order[0]]\n",
    "f2 = features[order[1]]\n",
    "d1 = x_train[rename[f1]].values.copy()\n",
    "d2 = x_train[rename[f2]].values.copy()\n",
    "s1 = train_shap_values.values[:, features.index(f1)].copy()\n",
    "s2 = train_shap_values.values[:, features.index(f2)].copy()\n",
    "i = 2*train_shap_interactions[:, features.index(f1), features.index(f2)]\n",
    "\n",
    "# prepare data\n",
    "x_dt = np.concatenate([d1[:, None], d2[:, None]], axis=1)\n",
    "y1 = (np.sign(s1) > 0).astype(int)\n",
    "y2 = (np.sign(s2) > 0).astype(int)\n",
    "yi = (np.sign(i) > 0).astype(int)\n",
    "\n",
    "# individual splits\n",
    "individual_splits = [\n",
    "    Metrics.optimal_shap_splits(x_dt[:, 0:1], y1, [f1])[1][0],\n",
    "    Metrics.optimal_shap_splits(x_dt[:, 1:2], y2, [f2])[1][0]]\n",
    "\n",
    "# interaction splits\n",
    "interaction_splits = Metrics.optimal_shap_splits(x_dt, yi, [f1, f2])[1]\n",
    "\n",
    "Plots.shap_interactions(\n",
    "    feature_names=[f1, f2],\n",
    "    feature_data=[d1, d2],\n",
    "    feature_shaps=[s1, s2],\n",
    "    feature_interactions=i,\n",
    "    individual_splits=individual_splits,\n",
    "    interaction_splits=interaction_splits,\n",
    "    rename=rename,\n",
    "    save_name=f'figures/{dataset}_shap_interactions.pdf',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c408439f",
   "metadata": {},
   "source": [
    "# Plot threshold model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d58d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# plot threshold model\n",
    "#\n",
    "\n",
    "# book keeping\n",
    "if dataset == 'env':\n",
    "    rules = [\n",
    "        lambda x, t: (x >= t).astype(int), \n",
    "        lambda x, t: (x >= t).astype(int)]\n",
    "if dataset == 'bat':\n",
    "    rules = [\n",
    "        lambda x, t: (x <= t).astype(int), \n",
    "        lambda x, t: (x > t).astype(int)]\n",
    "d1 = x_total[rename[f1]].values.copy()\n",
    "d2 = x_total[rename[f2]].values.copy()\n",
    "\n",
    "# plot threshold model\n",
    "Plots.threshold_model(\n",
    "    y_true=y,\n",
    "    dates=dates,\n",
    "    feature_names=[rename[f1], rename[f2]],\n",
    "    feature_data=[d1, d2],\n",
    "    feature_splits=interaction_splits,\n",
    "    feature_rules=rules,\n",
    "    train_idx=train_idx,\n",
    "    test_idx=test_idx,\n",
    "    dataset=dataset,\n",
    "    save_name=f'figures/{dataset}_threshold_model.pdf',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1529c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print individual-level metrics\n",
    "split1, split2 = individual_splits\n",
    "y_ind_test = rules[0](d1[test_idx], split1) * rules[1](d2[test_idx], split2)\n",
    "Metrics.print_metrics(\n",
    "    y, \n",
    "    np.concatenate([np.zeros(len(train_idx)), y_ind_test]),\n",
    "    dates.values, \n",
    "    threshold=0.5,\n",
    "    test_idx=test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4c7011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print interaction-level metrics\n",
    "split1, split2 = interaction_splits\n",
    "y_int_test = rules[0](d1[test_idx], split1) * rules[1](d2[test_idx], split2)\n",
    "Metrics.print_metrics(\n",
    "    y, \n",
    "    np.concatenate([np.zeros(len(train_idx)), y_int_test]),\n",
    "    dates.values, \n",
    "    threshold=0.5,\n",
    "    test_idx=test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572e3ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216e7292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c3c1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "80f03edaed8255d3091f821e6083baade26957ca8f6a3da5a154de536aea2ee6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
